{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80c164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8788dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../swarlekha_model/weights/conds.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a862e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7333a23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3 <class 'dict'>\n",
      "gen <class 'dict'>\n",
      "speaker_emb <class 'torch.Tensor'> torch.Size([1, 256])\n",
      "clap_emb <class 'NoneType'>\n",
      "cond_prompt_speech_tokens <class 'torch.Tensor'> torch.Size([1, 150])\n",
      "cond_prompt_speech_emb <class 'NoneType'>\n",
      "emotion_adv <class 'torch.Tensor'> torch.Size([1, 1, 1])\n",
      "prompt_token <class 'torch.Tensor'> torch.Size([1, 157])\n",
      "prompt_token_len <class 'torch.Tensor'> torch.Size([1])\n",
      "prompt_feat <class 'torch.Tensor'> torch.Size([1, 314, 80])\n",
      "prompt_feat_len <class 'NoneType'>\n",
      "embedding <class 'torch.Tensor'> torch.Size([1, 192])\n"
     ]
    }
   ],
   "source": [
    "for keys in model.keys():\n",
    "    print(keys, type(model[keys]))\n",
    "\n",
    "for keys in model.keys():\n",
    "    for key in model[keys].keys():\n",
    "        if isinstance(model[keys][key], torch.Tensor):\n",
    "            print(key, type(model[keys][key]), model[keys][key].shape)\n",
    "        else:\n",
    "            print(key, type(model[keys][key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63fbf2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire file\n",
    "s3_tensors = load_file(\"../swarlekha_model/weights/s3gen.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f2d14b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow.decoder.estimator.down_blocks.0.0.block1.block.0.bias <class 'torch.Tensor'> torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.attn1.to_q.weight <class 'torch.Tensor'> torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.0.block1.block.0.bias <class 'torch.Tensor'> torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.norm3.bias <class 'torch.Tensor'> torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.0.mlp.1.bias <class 'torch.Tensor'> torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.ff.net.0.proj.weight <class 'torch.Tensor'> torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.attn1.to_v.weight <class 'torch.Tensor'> torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.attn1.to_out.0.bias <class 'torch.Tensor'> torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.norm3.weight <class 'torch.Tensor'> torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.ff.net.0.proj.bias <class 'torch.Tensor'> torch.Size([1024])\n",
      "flow.encoder.encoders.4.self_attn.linear_k.bias <class 'torch.Tensor'> torch.Size([512])\n",
      "flow.encoder.up_encoders.3.norm_ff.weight <class 'torch.Tensor'> torch.Size([512])\n",
      "mel2wav.resblocks.2.convs1.0.bias <class 'torch.Tensor'> torch.Size([256])\n",
      "mel2wav.resblocks.6.convs1.1.parametrizations.weight.original0 <class 'torch.Tensor'> torch.Size([64, 1, 1])\n",
      "mel2wav.source_resblocks.1.convs1.0.parametrizations.weight.original1 <class 'torch.Tensor'> torch.Size([128, 128, 7])\n",
      "speaker_encoder.head.layer2.0.bn2.weight <class 'torch.Tensor'> torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd2.nonlinear2.batchnorm.weight <class 'torch.Tensor'> torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd9.cam_layer.linear2.weight <class 'torch.Tensor'> torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd14.nonlinear1.batchnorm.num_batches_tracked <class 'torch.Tensor'> torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd2.nonlinear2.batchnorm.bias <class 'torch.Tensor'> torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd3.nonlinear2.batchnorm.weight <class 'torch.Tensor'> torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd1.cam_layer.linear2.weight <class 'torch.Tensor'> torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd15.nonlinear1.batchnorm.num_batches_tracked <class 'torch.Tensor'> torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd6.nonlinear2.batchnorm.bias <class 'torch.Tensor'> torch.Size([128])\n",
      "tokenizer.encoder.blocks.0.mlp.2.weight <class 'torch.Tensor'> torch.Size([1280, 5120])\n",
      "Total tensors: 2489\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for keys in s3_tensors.keys():\n",
    "    if count % 100 == 0:\n",
    "        print(keys, type(s3_tensors[keys]), s3_tensors[keys].shape)\n",
    "    count += 1\n",
    "print(\"Total tensors:\", count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
